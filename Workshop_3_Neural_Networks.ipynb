{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Workshop_3_Neural_Networks.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"nMshRWBVDPBh","colab_type":"text"},"cell_type":"markdown","source":["#Introduction to Neural Networks"]},{"metadata":{"id":"blTnauO7lj32","colab_type":"text"},"cell_type":"markdown","source":["Neural Networks is the forefront of today's Machine Learning research. In this workshop, I will be going over how Neural Networks work and I will run over a hands-on example of using a Neural Network for Reinforcement Learning."]},{"metadata":{"id":"ZjaSsUyN6HMQ","colab_type":"text"},"cell_type":"markdown","source":["## Reinforcement Learning\n","\n","Today we will be covering Q-learning, a specific type of learning that Google utilized back in 1900's.\n","\n","### What is it used for?\n","* Playing Games (AI).\n","* Time series analysis (financial strategies).\n","* Robots (Manufacturing)\n","* Learning partially observable environments. Most problems you can apply POMDP to.\n","\n"]},{"metadata":{"id":"GBb6BqgWE5pB","colab_type":"text"},"cell_type":"markdown","source":["### How does it work?\n","\n","Q-learning is a specific type of Reinforcement learning with the following setup.\n","\n","\n","---\n","\n","\n","Input: Observations from the environment\n","\n","Output: Action to perform on the environment (optimal action-selection policy)\n","\n","What it learns: Learns the expected reward of a performing an action on a given state. We call these expected values \"policies\".\n","\n","\n","\n","---\n","\n","This may seem confusing, let's work through our example first.\n"]},{"metadata":{"id":"UExv4BgrMjcW","colab_type":"text"},"cell_type":"markdown","source":["In this case, at every position that the cart is in, we feed the x and y coordinates to the Neural Network as input.\n","\n","Then, Neural Network produces a value, which is the expected action to take on the environment given this x and y."]},{"metadata":{"id":"iUOlwmaHo3ZR","colab_type":"text"},"cell_type":"markdown","source":["## Installing Dependencies"]},{"metadata":{"id":"rXR15GjJl8Ft","colab_type":"text"},"cell_type":"markdown","source":["First off, let's import our libraries."]},{"metadata":{"id":"SHOfKTqTDAJl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["# Import libraries\n","import matplotlib.pyplot as plt\n","import random\n","import pandas as pd\n","import tensorflow as tf\n","import numpy as np\n","import random"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vdh8zv7Ck-YD","colab_type":"text"},"cell_type":"markdown","source":["### Installing OpenAI's gym\n","Today, we will be using OpenAI's gym environment. To install on your computer, run 'pip install gym' on your command line (highly recommended!). You should also ensure you have OpenGL installed on your computer."]},{"metadata":{"id":"lr7hzf7QlIRH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["# Run this if you do not have opengl\n","!apt-get install python-opengl\n","# Run this to install OpenAI gym\n","!pip install gym\n","import gym"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fCcESxz4o_nE","colab_type":"text"},"cell_type":"markdown","source":["## Exploring the Environment"]},{"metadata":{"id":"XcWtXx7uotoe","colab_type":"text"},"cell_type":"markdown","source":["Great, now that we have installed gym, let's try to explore what it does. \n","\n","First, let's load the environment called 'MountainCarContinuous-v0' and call reset() to initialize our setup."]},{"metadata":{"id":"jcB-ZmJw_JH5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["env = gym.make('CartPole-v1')\n","env.reset()\n","random.seed()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bUL5RVV6_Ls6","colab_type":"text"},"cell_type":"markdown","source":["For every action on the environment, the environment provides us a list of fields, called observations that results from the action we took.\n","\n","Let's take 5 actions and observe what we get back from the environment."]},{"metadata":{"id":"zrBvY-jBmwnQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["for step in range(5):\n","    action = env.action_space.sample()   # Generate a random action\n","    \n","    # Apply the predicted step onto the environment.\n","    # Receive back a observation, reward, done flag, and extra information\n","    obs1,reward,done,_ = env.step(action)\n","    \n","    print(\"STEP {}: We took an action {} and got back:\".format(step,action))\n","    print(\"Observation {}\".format(obs1))\n","    print(\"Reward: {} \\n\".format(reward))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4lR65UEH27Ml","colab_type":"text"},"cell_type":"markdown","source":["**Note**: *The input is an array of one element*\n","\n","What does this mean? Well, for every action we've taken, we got back the parameters for the new state of the environment. We call these \"Observations\". Along with the observations, we are given a \"Reward\".\n","\n","\n","---\n","\n","\n","### Question: \n","Why is Observation a matrix of dimension 4? What does each element in this matrix represent?\n","\n","\n","---\n","\n","\n","We will use the Observations as inputs to our Neural Network and we will use the Rewards for the loss function!\n","\n","We can use the following commands to find the maximum Action values and maximum Observation values."]},{"metadata":{"id":"LFPR-TNdDgF_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(env.action_space)\n","\n","print(env.observation_space.high)\n","print(env.observation_space.low)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jzELDPhGD2US","colab_type":"text"},"cell_type":"markdown","source":["Great, now we know the bound of our action: {0, 1} and the expected values for our observation!"]},{"metadata":{"id":"_FGXGjoxpE6X","colab_type":"text"},"cell_type":"markdown","source":["## Creating our Neural Network"]},{"metadata":{"id":"TBfFu36G7Tcv","colab_type":"text"},"cell_type":"markdown","source":["Let's start to create our Neural Network. First, let's get some variables from the environment.\n","\n","We can use this to determine how many input nodes we need and how many output nodes we need."]},{"metadata":{"id":"-LAKN_Km7bq2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(env.observation_space.shape)\n","INPUTSIZE = 4\n","\n","print(env.action_space.shape)\n","OUTPUTSIZE = 2\n","\n","# Number of hidden layers\n","HIDDENSIZE=15"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VPYTeH6_-N6w","colab_type":"text"},"cell_type":"markdown","source":["Tensorflow helps us build a graph of our Neural Network before we train the network. To start, let's reset the tensorflow graph."]},{"metadata":{"id":"INENWVC5-lSh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K0hHezQo-oU2","colab_type":"text"},"cell_type":"markdown","source":["### Input Layer:\n","\n","Next, let's use the function `tf.placeholder(dtype, shape)` to create our input layer. Notice that the shape of the input layer is: 1 x 2 (or 1 x `INPUTSIZE`)."]},{"metadata":{"id":"wy9H3LIl_Oip","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["inputs = tf.placeholder(tf.float32, shape=(1,INPUTSIZE))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zxP9k_7r_R3Z","colab_type":"text"},"cell_type":"markdown","source":["### Hidden Layers:\n","\n","Now let's create the weights and bias to connect the input and the first hiddlen layer. Let's first declare our variables."]},{"metadata":{"id":"uhgQkC6h_Noz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["W1 = tf.get_variable(\"W1\", shape=[INPUTSIZE, HIDDENSIZE], initializer=tf.contrib.layers.xavier_initializer())   # A 2 x 10 layer\n","b1 = tf.Variable(tf.zeros(HIDDENSIZE))   # Initialize bias with 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HyZw2BFfANq7","colab_type":"text"},"cell_type":"markdown","source":["Let's add both `W1` and `b1` together in a layer and call it `layer1`. \n","\n","`tf.matmul` multiplies inputs and W1 together. `tf.nn.relu` applies the Rectified Linear function to the equation."]},{"metadata":{"id":"3UTAJ3zSANEh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["layer1 = tf.nn.relu(tf.matmul(inputs, W1)+b1)   # Use ReLU activation function"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QkFwOEslB6nz","colab_type":"text"},"cell_type":"markdown","source":["Now let's do the same thing with the second layer. This time, the size of the layer is 10x10. In otherwords, `INPUTSIZE` x `INPUTSIZE`"]},{"metadata":{"id":"qIncEStnCAFh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["W2 = tf.get_variable(\"W2\", shape=[HIDDENSIZE, HIDDENSIZE], initializer=tf.contrib.layers.xavier_initializer())\n","b2 = tf.Variable(tf.zeros(HIDDENSIZE))\n","layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EP4E-breCotl","colab_type":"text"},"cell_type":"markdown","source":["### Output Layer:\n","\n","Now it's time to create our output layer of size 10 x 1."]},{"metadata":{"id":"xXcupsdrCs0y","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["W3 = tf.get_variable(\"W3\", shape=[HIDDENSIZE, OUTPUTSIZE], initializer=tf.contrib.layers.xavier_initializer())\n","layer_out= tf.matmul(layer2, W3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MAiSm7jjDHi3","colab_type":"text"},"cell_type":"markdown","source":["The last layer, `layer_out` will produce a value, however, not neccessarily an action. We apply argmax, to get the index of action of the largest expected reward."]},{"metadata":{"id":"MWMPkGOZDK6F","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Output: returns a value either 0 or 1\n","predict = tf.argmax(layer_out,1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PUx6dq9PXzoD","colab_type":"text"},"cell_type":"markdown","source":["This prediction will be what we feed into the environment to get the next Observation!\n","\n","\n","---\n","\n","\n","After each action, we receive the reward from the environment. With the reward, we need to define a loss function to evaluate our Neural Network's performance.\n","Here we will use least squares for our loss function. There are other loss functions that we can use."]},{"metadata":{"id":"Jx0i_JLOC6Ku","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["rewardQ = tf.placeholder(shape=[1,OUTPUTSIZE],dtype=tf.float32)\n","loss = tf.losses.mean_squared_error(rewardQ, layer_out)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xgn2WYZY-DAp","colab_type":"text"},"cell_type":"markdown","source":["Now, let's are declare our optimizer. A good choice in optimizer and learning rate can affect the time it takes to converge. Here we will use the Adam Optimizer, which is an optimizer extended from gradient descent. In practice, AdamOptimizer generally performs better than GradientDescentOptimizer. We also declare updateModel so we can have tensorflow minimize the loss function w.r.t. the optimizer."]},{"metadata":{"id":"OLGFNns3-DAq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["trainer = tf.train.AdamOptimizer(learning_rate=0.01)\n","updateModel = trainer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YKq_2wJV-DAr","colab_type":"text"},"cell_type":"markdown","source":["Finally, let's declare a initializer. `init` will activate the initializers in all the variables."]},{"metadata":{"id":"guMSDE1w-DAr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Initialize the weights and biases\n","init = tf.global_variables_initializer()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X-8f8EV5AJgj","colab_type":"text"},"cell_type":"markdown","source":["## Training the Network"]},{"metadata":{"id":"ttw7iXVl-DAt","colab_type":"text"},"cell_type":"markdown","source":["Now, we can train our Neural Network.\n","\n","First, let's set a few parameters for our Neural Network."]},{"metadata":{"id":"8x0qOUmr-DAt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Set learning parameters\n","gamma = 0.99\n","epsilon = 0.05\n","num_episodes = 400\n","maxsteps=500\n","\n","# For graphing rewards\n","rList=[]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1EkkWP9f-DAv","colab_type":"text"},"cell_type":"markdown","source":["We create a new Tensorflow session. These session instances keep track of things like Variables specific to the session."]},{"metadata":{"id":"wOfcLsVN-DAv","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sess = tf.Session()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zBWKpwQH-DAx","colab_type":"text"},"cell_type":"markdown","source":["As an example, let's use sess to call `init` and initialize our weights."]},{"metadata":{"id":"GF5cq4xt-DAx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sess.run(init)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ce7AanUj-DAz","colab_type":"text"},"cell_type":"markdown","source":["Great! Now all the weights are randomly assigned. Now, we can train our network."]},{"metadata":{"id":"1bwVahrW-DAz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["obs=env.reset()\n","rAll, steps = 0, 0\n","done = False\n","while steps < maxsteps:\n","    # Run the network with obs, and received the values for predict and layer_out\n","    action, actionQs = sess.run([predict, layer_out], feed_dict={inputs:obs.reshape(1,INPUTSIZE)})\n","\n","    # Epsilon greedy exploration\n","    if np.random.rand(1) < epsilon:\n","        action[0] = env.action_space.sample()\n","\n","    # Apply the predicted step onto the environment.\n","    # Receive back a observation, reward, done flag, and information\n","    next_obs,reward,done,_ = env.step(action[0])\n","\n","    # Evaluate the neural network on next_obs: the observation after the action\n","    evalQ = sess.run(layer_out, feed_dict={inputs:next_obs.reshape(1,INPUTSIZE)})\n","    \n","    #Take the largest value of all the next Q(s',a') values\n","    max_evalQ = np.max(evalQ)\n","    # Grab the old Q function\n","    targetQ = actionQs\n","    \n","    # Update the (old) Q(s,a) value\n","    if done:\n","        targetQ[0,action[0]] = reward\n","    else:\n","        targetQ[0,action[0]] = reward + gamma*max_evalQ\n","\n","    # This will train our network with the update Q(s,a) value\n","    # and update the weights once every episode.\n","    sess.run(updateModel, feed_dict={inputs:obs.reshape(1,INPUTSIZE),rewardQ:targetQ})\n","    # Change the state s=s'\n","    obs=next_obs\n","\n","    # Add to the steps and record the reward for graphs\n","    steps += reward\n","    if done:\n","        rAll += steps\n","        rList.append(rAll)\n","        break;"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cB6gHJim-DA1","colab_type":"text"},"cell_type":"markdown","source":["Cool! Let's see how it did."]},{"metadata":{"id":"JTiYhd8r-DA1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(\"Episode: 1 Steps: \"+str(steps))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-EGUf07k-DA4","colab_type":"text"},"cell_type":"markdown","source":["More than likely, the neural network did not perform very well! Let's try to hit 500 steps by training it for 500 more iteraions."]},{"metadata":{"id":"FVFoYZso-DA4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["for i in range(num_episodes):\n","    obs=env.reset()\n","    rAll, steps = 0, 0\n","    done = False\n","    while steps < maxsteps:\n","        #if i % 50 == 0:\n","            #env.render()\n","            \n","        action, actionQs = sess.run([predict, layer_out], feed_dict={inputs:obs.reshape(1,INPUTSIZE)})\n","\n","        if np.random.rand(1) < epsilon:\n","            action[0] = env.action_space.sample()\n","            \n","        next_obs,reward,done,_ = env.step(action[0])\n","\n","        evalQ = sess.run(layer_out, feed_dict={inputs:next_obs.reshape(1,INPUTSIZE)})\n","        max_evalQ = np.max(evalQ)\n","        targetQ = actionQs\n","\n","        if done:\n","            targetQ[0,action[0]] = reward\n","        else:\n","            targetQ[0,action[0]] = reward + gamma*max_evalQ\n","\n","        sess.run(updateModel, feed_dict={inputs:obs.reshape(1,INPUTSIZE),rewardQ:targetQ})\n","        obs=next_obs\n","\n","        steps += reward\n","        if done:\n","            rAll += steps\n","            rList.append(rAll)\n","            break;\n","    print \"Episode: {}/{} Steps: {}\".format(i,num_episodes,rAll)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hDD_Cw7b-DA6","colab_type":"text"},"cell_type":"markdown","source":["Time to create a graph of how we did:"]},{"metadata":{"id":"0Mvvdwag-DA6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print \"Average steps per episode: \" + str(sum(rList)/num_episodes) + \" steps\"\n","plt.plot(rList)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5BOaOhe4-DA8","colab_type":"text"},"cell_type":"markdown","source":["Did it learn? Perhaps did the weights diverge? What happened?\n","\n","Let's test it out on the environment"]},{"metadata":{"id":"3yT9QOMm-DA9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["obs=env.reset()\n","steps = 0\n","done = False\n","while steps < maxsteps:\n","    #env.render()\n","\n","    actions = sess.run(predict, feed_dict={inputs:obs.reshape(1,INPUTSIZE)})\n","    obs,reward,done,_ = env.step(actions[0])\n","    if done:\n","        print(\"Performed \"+str(steps))\n","        break\n","    steps += reward"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AO1oOMnx-DA-","colab_type":"text"},"cell_type":"markdown","source":["We are now done with the environment. Let's close the session."]},{"metadata":{"id":"-MDRqR5f-DA_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sess.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bo4bR6QB-DA_","colab_type":"text"},"cell_type":"markdown","source":["Now you've created your (maybe first) neural network!\n","\n","Where can we go from here?<br>\n","Keras for DQN: https://keon.io/deep-q-learning/ <br>\n","Convolutional Neural Networks: http://cs231n.github.io/convolutional-networks/ <br>\n","Long-Short Term Memory (LSTMs): http://colah.github.io/posts/2015-08-Understanding-LSTMs/ <br>\n","Deep Learning Specialization: https://www.coursera.org/specializations/deep-learning<br>\n","Loss functions in NNs: https://isaacchanghau.github.io/2017/06/07/Loss-Functions-in-Artificial-Neural-Networks/"]}]}